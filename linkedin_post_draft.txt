ðŸš€ **Engineering a Portable, Context-Aware AI Assistant in C++**

Iâ€™ve just open-sourced **QueryMate**, a specialized CLI assistant built on top of the incredible `llama.cpp` library.

**ðŸ’¡ The Inspiration**
Heavily inspired by the workflow and utility of the **Gemini CLI**, my goal was to bring that same seamless, developer-centric experience to a completely local, offline environment. I wanted the polish of a cloud-connected CLI tool with the privacy and control of local execution.

ðŸ¤– **Why not just use the bare `llama-cli`?**
While `llama.cpp` provides state-of-the-art inference performance, using the raw CLI examples often feels like interacting with a raw engine block. QueryMate bridges the gap between **raw inference** and a **productive workflow tool**.

It wraps the low-level C API in a robust, object-oriented C++ architecture to solve common usability friction points:

1.  **Dynamic Context Engineering:** Instead of manually pasting text or piping generic inputs, QueryMate implements a custom tokenizer and parser. You can inject specific codebases or documents dynamically using an innovative `@filename` syntax (e.g., *"Refactor the logic in @main.cpp"*), which the system automatically parses and embeds into the context window.

2.  **Stateful Memory Management:** Unlike stateless inference calls, QueryMate features a dedicated `MemoryManager`. It handles sliding context windows and conversation history persistence automatically, ensuring long conversations don't break the model's context limit or crash the session.

3.  **True "USB Portability":** This is a zero-dependency binary. No Python environments, no pip installs, no Docker containers. You can compile it, throw it on a **USB drive** with your GGUF models, and plug it into an air-gapped or offline machine to have a fully functional, private coding assistant instantly.

4.  **Hot-Swappable Architecture:** A structured `ModelManager` allows for runtime switching between models (e.g., swapping from `Llama-3` for reasoning to `Codestral` for programming) without restarting the application shell.

This project was an exercise in modern C++ design patterns (RAII, smart pointers) applied to AI engineering.

Check out the source code below! ðŸ‘‡

[Insert GitHub Link Here]

#CPP #MachineLearning #LlamaCpp #GeminiAI #LLM #GenerativeAI #EmbeddedSystems #SoftwareArchitecture #OfflineAI
